{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lanlab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General outlook of the framework's structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lanlab is a framework that aims at providing convenient and simple tools to help researchers build their own language model pipeline. As research in language model is developing rapidly, it is important to have a flexible framework that can be easily extended to support new models and new tasks. Lanlab is built around very simple and general concepts, and it is easy to extend it to support new models and new tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objects in lanlab are either **stuctures** or **modules** that operate on the stuctures. \n",
    "\n",
    "In native lanlab, there are three types of structures :\n",
    "- Segment\n",
    "- Sequence (lists of segments)\n",
    "- Batch (array of sequences)\n",
    "\n",
    "Let's start with the basics : Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flask in c:\\users\\nicol\\miniconda3\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in c:\\users\\nicol\\miniconda3\\lib\\site-packages (from flask) (3.0.1)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in c:\\users\\nicol\\miniconda3\\lib\\site-packages (from flask) (2.1.2)\n",
      "Requirement already satisfied: blinker>=1.6.2 in c:\\users\\nicol\\miniconda3\\lib\\site-packages (from flask) (1.7.0)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in c:\\users\\nicol\\miniconda3\\lib\\site-packages (from flask) (3.1.2)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\users\\nicol\\miniconda3\\lib\\site-packages (from flask) (8.1.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\nicol\\miniconda3\\lib\\site-packages (from click>=8.1.3->flask) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nicol\\miniconda3\\lib\\site-packages (from Jinja2>=3.1.2->flask) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text creates a sequence with one segment containing the given text\n",
      "len(my_text) =  1\n",
      "my_text[0].__class__ =  <class 'lanlab.core.structure.segment.Segment'>\n",
      "my_text[0].format() =  I like chocolate\n",
      "\n",
      "The format() method returns a formatting of the text of the segment but it can also be used on sequences concatenating the formatting of all its segments\n",
      "my_text.format() =  I like chocolate\n"
     ]
    }
   ],
   "source": [
    "#Creating text segments\n",
    "from lanlab import Text\n",
    "\n",
    "my_text = Text(\"I like chocolate\")\n",
    "\n",
    "print(\"Text creates a sequence with one segment containing the given text\")\n",
    "print(\"len(my_text) = \", len(my_text))\n",
    "print(\"my_text[0].__class__ = \", my_text[0].__class__)\n",
    "print(\"my_text[0].format() = \", my_text[0].format())\n",
    "print()\n",
    "print(\"The format() method returns a formatting of the text of the segment but it can also be used on sequences concatenating the formatting of all its segments\")\n",
    "print(\"my_text.format() = \", my_text.format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts can be concatenated with the + operator\n",
      "my_text3 = my_text + my_text2\n",
      "my_text3.format() =  I like chocolate and I like ice cream\n",
      "\n",
      "my_text3 contains 2 segments\n",
      "len(my_text3) =  2\n",
      "my_text3[0].format() =  I like chocolate\n",
      "my_text3[1].format() =   and I like ice cream\n"
     ]
    }
   ],
   "source": [
    "#Concatenating segments\n",
    "my_text2 = Text(\" and I like ice cream\")\n",
    "my_text3 = my_text + my_text2\n",
    "\n",
    "print(\"Texts can be concatenated with the + operator\")\n",
    "print(\"my_text3 = my_text + my_text2\")\n",
    "print(\"my_text3.format() = \", my_text3.format())\n",
    "\n",
    "print()\n",
    "print(\"my_text3 contains 2 segments\")\n",
    "print(\"len(my_text3) = \", len(my_text3))\n",
    "print(\"my_text3[0].format() = \", my_text3[0].format())\n",
    "print(\"my_text3[1].format() = \", my_text3[1].format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Editing segments data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segments store a lot of data more than text ['text', 'origin', 'tags', 'info', 'format', 'model', 'tokens', 'logp', 'top_logp', 'finish_reason', 'logits']\n",
      "segment['text'] =  I like chocolate -- is the text of the segment\n",
      "segment['format'] =  {'chat': '[text]', 'completion': '[text]'} -- is the formatting system of the segment. LLMs will get the completion formatting or chat formatting depending on their nature. This formating will just replace the [text] with the segment's text\n",
      "\n",
      "Let's update it : {'chat': '{[text]}', 'completion': '[text]'}\n",
      "segment.format('completion') =  I like chocolate -- is the completion formatting\n",
      "segment.format('chat') =  {I like chocolate} -- is the chat formatting\n",
      "This makes it possible to use the same segment and to adapt the prompting for chat and completion models when needed\n",
      "\n",
      "segment['origin'] =  user -- is the origin of the segment. It is about who generated this segment. It can be a user, a model or the system. This will then be communicated to chat models. This information isn't used by completion models\n",
      "segment['model'] =  None -- is the name of the model that generated this segment. It is put to None if the user created it\n",
      "\n",
      "There are many more that will be discussed later as they are not important for now\n"
     ]
    }
   ],
   "source": [
    "#Segment data\n",
    "segment = my_text3[0]\n",
    "\n",
    "print(\"Segments store a lot of data more than text\",segment.keys())\n",
    "print(\"segment['text'] = \", segment['text'],\"-- is the text of the segment\")\n",
    "print(\"segment['format'] = \", segment['format'],\"-- is the formatting system of the segment. LLMs will get the completion formatting or chat formatting depending on their nature. This formating will just replace the [text] with the segment's text\")\n",
    "\n",
    "print()\n",
    "segment['format']['chat'] = '{[text]}'\n",
    "print(\"Let's update it :\",segment['format'])\n",
    "print(\"segment.format('completion') = \", segment.format('completion'),\"-- is the completion formatting\")\n",
    "print(\"segment.format('chat') = \", segment.format('chat'),\"-- is the chat formatting\")\n",
    "print(\"This makes it possible to use the same segment and to adapt the prompting for chat and completion models when needed\")\n",
    "\n",
    "print()\n",
    "print(\"segment['origin'] = \", segment['origin'], \"-- is the origin of the segment. It is about who generated this segment. It can be a user, a model or the system. This will then be communicated to chat models. This information isn't used by completion models\")\n",
    "print(\"segment['model'] = \", segment['model'], \"-- is the name of the model that generated this segment. It is put to None if the user created it\")\n",
    "\n",
    "print()\n",
    "print(\"There are many more that will be discussed later as they are not important for now\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules can be called on sequences to complete them with an additional segment\n",
      "new_text = model(my_text3) =  I like chocolate and I like ice creamThat's great! Chocolate and ice cream make a delicious combination.\n",
      "\n",
      "Chats are not very readable with the format method as it concatenates all the segments. Let's use the show method instead to print a human readable version of the sequence\n",
      "print(new_text) = \n",
      "user(None): I like chocolate\n",
      "user(None):  and I like ice cream\n",
      "assistant(gpt-3.5-turbo-0613): That's great! Chocolate and ice cream make a delicious combination.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calling a model\n",
    "from lanlab import GPT35\n",
    "model = GPT35()\n",
    "\n",
    "print(\"Modules can be called on sequences to complete them with an additional segment\")\n",
    "my_text4 = model(my_text3)\n",
    "print(\"new_text = model(my_text3) = \", my_text4.format())\n",
    "\n",
    "print()\n",
    "print(\"Chats are not very readable with the format method as it concatenates all the segments. Let's use the show method instead to print a human readable version of the sequence\")\n",
    "print(\"print(new_text) = \")\n",
    "print(my_text4.show())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some clarifications are needed here to explain how lanlab handles text and present them to the models. For completion models, the model will get as input q string : ```sequence.format(\"completion\")```. For chat models a chat is required. The ```segment.format(\"chat\")``` for the content along with ```segment.['origin']``` for the origin of the text are used to feed the model. Each segment is a message, a sequence being a conversation. The conversation can include system messages as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuring a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules have their own configurations like segments\n",
      "model.config.keys() =  ['temperature', 'max_tokens', 'top_p', 'stop', 'logit_bias']\n",
      "\n",
      "model.config['max_tokens'] =  16 -- is the maximum number of tokens of the generated text\n",
      "model.config['temperature'] =  0.7 -- is the temperature of the generated text\n",
      "model.config['top_p'] =  1 -- is the top_p of the generated text\n",
      "The other parameters are more complex and will be discussed later\n"
     ]
    }
   ],
   "source": [
    "#Configuring a model\n",
    "print(\"Modules have their own configurations like segments\")\n",
    "print(\"model.config.keys() = \", model.config.keys())\n",
    "\n",
    "print()\n",
    "print(\"model.config['max_tokens'] = \", model.config['max_tokens'], \"-- is the maximum number of tokens of the generated text\")\n",
    "print(\"model.config['temperature'] = \", model.config['temperature'], \"-- is the temperature of the generated text\")\n",
    "print(\"model.config['top_p'] = \", model.config['top_p'], \"-- is the top_p of the generated text\")\n",
    "\n",
    "print(\"The other parameters are more complex and will be discussed later\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In native lanlab there are 3 preimplemented bridges to other framework : OpenAI, HuggingFace and Palm (coming soon)\n",
      "In the future more will be added and the user is free to add his own\n",
      "The available models are :\n",
      "OPENAI\n",
      "  ada\n",
      "  text-ada-001\n",
      "  babbage\n",
      "  text-babbage-001\n",
      "  curie\n",
      "  text-curie-001\n",
      "  davinci\n",
      "  davinci-instruct-beta\n",
      "  text-davinci-001\n",
      "  code-davinci-002\n",
      "  text-davinci-002\n",
      "  text-davinci-003\n",
      "  gpt-3.5-turbo-instruct\n",
      "  gpt-3.5-turbo-instruct-0914\n",
      "  davinci-002\n",
      "  babbage-002\n",
      "  gpt-3.5-turbo\n",
      "  gpt-3.5-turbo-0613\n",
      "  gpt-3.5-turbo-0301\n",
      "  gpt-3.5-turbo-16k\n",
      "  gpt-3.5-turbo-16k-0613\n",
      "  gpt-4\n",
      "  gpt-4-0314\n",
      "  gpt-4-0613\n",
      "  gpt-4-1106-preview\n",
      "  gpt-4-vision-preview\n",
      "\n",
      "HuggingFace\n",
      "  None\n",
      "  llama-7b\n",
      "  llama-13b\n",
      "  alpaca-7b\n",
      "  wizard-7b\n",
      "  vicuna-7b-v1.1\n",
      "  vicuna-7b-v1.3\n",
      "  vicuna-7b-v1.5\n",
      "  vicuna-13b-v1.1\n",
      "  vicuna-13b-v1.3\n",
      "  vicuna-13b-v1.5\n",
      "  baize-7b\n",
      "  guanaco-7b\n",
      "  tiny-llama-fast-tokenizer\n",
      "  llama-2-7b\n",
      "  llama-2-13b\n",
      "  llama-2-7b-hf\n",
      "  llama-2-13b-hf\n",
      "  Orca-2-7b\n",
      "  Orca-2-13b\n",
      "  None\n",
      "  bloom-3b\n",
      "  bloom-7b\n",
      "  bloomz-3b\n",
      "  bloomz-7b\n",
      "  None\n",
      "  pythia-2.8b\n",
      "  pythia-6.9b\n",
      "  pythia-12b\n",
      "  dolly-v2-3b\n",
      "  dolly-v2-7b\n",
      "  dolly-v2-12b\n",
      "  None\n",
      "  phi-1\n",
      "  phi-1_5\n",
      "  None\n",
      "  cerebras-111m\n",
      "  cerebras-256m\n",
      "  cerebras-590m\n",
      "  cerebras-1.3b\n",
      "  cerebras-2.7b\n",
      "  cerebras-6.7b\n",
      "  cerebras-13b\n",
      "  None\n",
      "  Qwen-1_8B\n",
      "  Qwen-7B\n",
      "  Qwen-14B\n",
      "  Qwen-72B\n",
      "  None\n",
      "  Yi-6B\n",
      "  Yi-34B\n",
      "  None\n",
      "  Mistral-7B-v0.1\n",
      "  Mistral-7B-Instruct-v0.1\n",
      "  zephyr-7b-alpha\n",
      "  zephyr-7b-beta\n",
      "  None\n",
      "  fuyu-8b\n",
      "  None\n",
      "  falcon-rw-1b\n",
      "  falcon-rw-7b\n",
      "  falcon-7b\n",
      "  falcon-7b-instruct\n",
      "  falcon-40b\n",
      "  falcon-40b-instruct\n",
      "  None\n",
      "  opt-125m\n",
      "  opt-350m\n",
      "  opt-1.3b\n",
      "  opt-2.7b\n",
      "  opt-6.7b\n",
      "  opt-13b\n",
      "  opt-30b\n",
      "  opt-66b\n",
      "  None\n",
      "  openchat_v2\n",
      "  openchat_v2_w\n",
      "  openchat_v3.1\n",
      "  openchat_v3.2\n",
      "  openchat_v3.2_super\n",
      "  openchat_3.5\n",
      "  None\n",
      "  Starling-LM-7B-alpha\n",
      "  None\n",
      "  neural-chat-7b-v3\n",
      "  neural-chat-7b-v3-1\n",
      "  None\n",
      "  causallm-7b\n",
      "  causallm-14b\n",
      "  None\n",
      "  tigerbot-7b-base\n",
      "  tigerbot-7b-base-v1\n",
      "  tigerbot-7b-base-v2\n",
      "  tigerbot-7b-sft-v1\n",
      "  tigerbot-7b-sft-v2\n",
      "  tigerbot-7b-chat\n",
      "  tigerbot-13b-base-v1\n",
      "  tigerbot-13b-base-v2\n",
      "  tigerbot-13b-chat-v1\n",
      "  tigerbot-13b-chat-v2\n",
      "  tigerbot-13b-chat-v3\n",
      "  tigerbot-13b-chat-v4\n",
      "  OpenHermes-7B\n",
      "  OpenHermes-13B\n",
      "  OpenHermes-2-Mistral-7B\n",
      "  OpenHermes-2.5-Mistral-7B\n",
      "  None\n",
      "  NexusRaven-13B\n",
      "  NexusRaven-V2-13B\n",
      "\n",
      "The classes can be used to create new models and are referenced in lanlab.get_openai_model_classes() and lanlab.get_hf_model_classes()\n",
      "Please note that OPENAI models access require an API key and HuggingFace models require hosting them on your own computer with all their own requirements\n"
     ]
    }
   ],
   "source": [
    "#Available models\n",
    "from lanlab import get_hf_model_classes, get_openai_model_classes\n",
    "print(\"In native lanlab there are 3 preimplemented bridges to other framework : OpenAI, HuggingFace and Palm (coming soon)\")\n",
    "print(\"In the future more will be added and the user is free to add his own\")\n",
    "print(\"The available models are :\")\n",
    "print(\"OPENAI\")\n",
    "for m in get_openai_model_classes():\n",
    "    print(\" \",m().engine)\n",
    "\n",
    "print()\n",
    "print(\"HuggingFace\")\n",
    "for m in get_hf_model_classes():\n",
    "    print(\" \",m().engine)\n",
    "\n",
    "print()\n",
    "print(\"The classes can be used to create new models and are referenced in lanlab.get_openai_model_classes() and lanlab.get_hf_model_classes()\")\n",
    "print(\"Please note that OPENAI models access require an API key and HuggingFace models require hosting them on your own computer with all their own requirements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches are arrays of sequences\n",
      "batch.shape =  (5, 2)\n",
      "batch[0,0] =  Sequence(\n",
      ")\n",
      "\n",
      "Batched sequences can be accessed with the same syntax as numpy arrays and can be filled with your own sequences\n",
      "batch[0,0] =  I like chocolate\n",
      "Models can be called on top of batches and will process sequences in parallel up to a given batch size set_number_workers(n). This speeds up the process a lot.\n",
      "batch_completed[ 0 , 0 ] =  0+0=0\n",
      "batch_completed[ 0 , 1 ] =  0+1=The sum of 0 and 1 is 1.\n",
      "batch_completed[ 1 , 0 ] =  1+0=1\n",
      "batch_completed[ 1 , 1 ] =  1+1=2\n",
      "batch_completed[ 2 , 0 ] =  2+0=2\n",
      "batch_completed[ 2 , 1 ] =  2+1=3\n",
      "batch_completed[ 3 , 0 ] =  3+0=3\n",
      "batch_completed[ 3 , 1 ] =  3+1=4\n",
      "batch_completed[ 4 , 0 ] =  4+0=4+0=4\n",
      "batch_completed[ 4 , 1 ] =  4+1=5\n"
     ]
    }
   ],
   "source": [
    "#Batches\n",
    "from lanlab import Batch,set_number_workers\n",
    "print(\"Batches are arrays of sequences\")\n",
    "batch = Batch((5,2)) #Creates a batch of 5x2 empty sequences\n",
    "print(\"batch.shape = \", batch.shape)\n",
    "print(\"batch[0,0] = \", batch[0,0])\n",
    "\n",
    "print()\n",
    "print(\"Batched sequences can be accessed with the same syntax as numpy arrays and can be filled with your own sequences\")\n",
    "batch[0,0] = my_text\n",
    "print(\"batch[0,0] = \", batch[0,0].format())\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(2):\n",
    "        batch[i,j] = Text(str(i)+\"+\"+str(j)+\"=\")\n",
    "\n",
    "set_number_workers(10) #Set the number of queries to be sent in parallel to the model\n",
    "print(\"Models can be called on top of batches and will process sequences in parallel up to a given batch size set_number_workers(n). This speeds up the process a lot.\")\n",
    "batch_completed = model(batch)\n",
    "for i in range(5):\n",
    "    for j in range(2):\n",
    "        print(\"batch_completed[\",i,\",\",j,\"] = \", batch_completed[i,j].format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential is a container that can be used to chain modules\n",
      "run\n",
      "results[ 0 , 0 ] = \n",
      " user(None): 0+0=\n",
      "assistant(gpt-3.5-turbo-0613): 0\n",
      "user(None): Are you sure ?\n",
      "assistant(gpt-3.5-turbo-0301): Yes, I am sure. Any number added to 0 is equal to that number itself. In this case, 0 added to 0 is equal to\n",
      "\n",
      "\n",
      "results[ 0 , 1 ] = \n",
      " user(None): 0+1=\n",
      "assistant(gpt-3.5-turbo-0613): 1\n",
      "user(None): Are you sure ?\n",
      "assistant(gpt-3.5-turbo-0301): Yes, I am sure. 0 plus 1 is equal to 1.\n",
      "\n",
      "\n",
      "results[ 1 , 0 ] = \n",
      " user(None): 1+0=\n",
      "assistant(gpt-3.5-turbo-0613): 1\n",
      "user(None): Are you sure ?\n",
      "assistant(gpt-3.5-turbo-0301): Yes, as an AI language model, I am certain that 1 plus 0 equals 1.\n",
      "\n",
      "\n",
      "results[ 1 , 1 ] = \n",
      " user(None): 1+1=\n",
      "assistant(gpt-3.5-turbo-0613): 2\n",
      "user(None): Are you sure ?\n",
      "assistant(gpt-3.5-turbo-0301): Yes, I am absolutely sure that 1+1=2. This is a basic mathematical fact that is universally accepted.\n",
      "\n",
      "\n",
      "results[ 2 , 0 ] = \n",
      " user(None): 2+0=\n",
      "assistant(gpt-3.5-turbo-0613): 2\n",
      "user(None): Are you sure ?\n",
      "assistant(gpt-3.5-turbo-0301): Yes, as an AI language model, I'm programmed to perform basic arithmetic operations accurately. Therefore, I can confirm that 2+0=2.\n",
      "\n",
      "\n",
      "results[ 2 , 1 ] = \n",
      " user(None): 2+1=\n",
      "assistant(gpt-3.5-turbo-0613): 3\n",
      "user(None): Are you sure ?\n",
      "assistant(gpt-3.5-turbo-0301): As an AI language model, I can confirm that 2+1 is indeed equal to 3.\n",
      "\n",
      "\n",
      "results[ 3 , 0 ] = \n",
      " user(None): 3+0=\n",
      "assistant(gpt-3.5-turbo-0613): 3\n",
      "user(None): Are you sure ?\n",
      "assistant(gpt-3.5-turbo-0301): Yes, as an AI language model, I am certain that 3 added to 0 equals 3.\n",
      "\n",
      "\n",
      "results[ 3 , 1 ] = \n",
      " user(None): 3+1=\n",
      "assistant(gpt-3.5-turbo-0613): 4\n",
      "user(None): Are you sure ?\n",
      "assistant(gpt-3.5-turbo-0301): Yes, as an AI language model, I can confirm that 3+1 equals 4.\n",
      "\n",
      "\n",
      "results[ 4 , 0 ] = \n",
      " user(None): 4+0=\n",
      "assistant(gpt-3.5-turbo-0613): 4\n",
      "user(None): Are you sure ?\n",
      "assistant(gpt-3.5-turbo-0301): Yes, as an AI language model, I am absolutely sure that 4+0 equals 4.\n",
      "\n",
      "\n",
      "results[ 4 , 1 ] = \n",
      " user(None): 4+1=\n",
      "assistant(gpt-3.5-turbo-0613): 5\n",
      "user(None): Are you sure ?\n",
      "assistant(gpt-3.5-turbo-0301): Yes, as an AI language model, I am absolutely certain that 4+1=5.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Sequential\n",
    "from lanlab import Sequential\n",
    "print(\"Sequential is a container that can be used to chain modules\")\n",
    "from lanlab import GPT35_0613, GPT35_0301\n",
    "\n",
    "#This will first call the model, then append the text \"Are you sure ?\" to the completion and then call the model again. TO make it more convenient, despite text not being a module it is interpreted as a module that concatenates the text to the sequence.\n",
    "model1 = GPT35_0613()\n",
    "model2 = GPT35_0301().configure(max_tokens=32)\n",
    "seq = Sequential(model1, \"Are you sure ?\", model2)\n",
    "\n",
    "results = seq(batch)\n",
    "for i in range(5):\n",
    "    for j in range(2):\n",
    "        print(\"results[\",i,\",\",j,\"] = \\n\", results[i,j].show())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n"
     ]
    }
   ],
   "source": [
    "from lanlab import save, load\n",
    "import os\n",
    "\n",
    "#Saving a structure (segment, sequence, batch)\n",
    "save(batch,os.path.join(\"tutorial_files\",\"my_batch\"))\n",
    "\n",
    "#Loading a structure (segment, sequence, batch)\n",
    "loaded_batch = load(os.path.join(\"tutorial_files\",\"my_batch\"))\n",
    "\n",
    "#Saving automatically in Sequential\n",
    "seq(batch,os.path.join(\"tutorial_files\",\"my_seq2\"))\n",
    "\n",
    "#If you run it again, it will load the saved file instead of running the model again\n",
    "results = seq(batch,os.path.join(\"tutorial_files\",\"my_seq2\")) #Another computation has the same path -> load it instead of running it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading HF models\n",
    "\n",
    "To run this cell you need to ```git clone https://huggingface.co/fxmarty/tiny-llama-fast-tokenizer``` and to follow the instructions in the lanlab README repository to install the model. This is a dummy model that completes with random token but is very convenient to verify that the pipeline works before running it on larger models on a GPU cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:httpx:load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "DEBUG:httpx:load_verify_locations cafile='c:\\\\Users\\\\nicol\\\\miniconda3\\\\lib\\\\site-packages\\\\certifi\\\\cacert.pem'\n",
      "INFO:root:starting flask server\n",
      "INFO:root:starting model hosting process\n",
      "DEBUG:root:creating workers\n",
      "DEBUG:root:creating workers b1\n",
      "DEBUG:root:fill input queue\n",
      "DEBUG:root:starting workers\n",
      "DEBUG:root:get results\n",
      "DEBUG:root:get results b\n",
      "DEBUG:root:looping get results\n",
      "DEBUG:root:loop iter get results\n",
      "DEBUG:root:loop iter get results\n",
      "DEBUG:root:loop iter get results\n",
      "DEBUG:root:loop iter get results\n",
      "DEBUG:root:loop iter get results\n",
      "DEBUG:root:loop iter get results\n",
      "DEBUG:root:loop iter get results\n",
      "DEBUG:root:loop iter get results\n",
      "DEBUG:root:loop iter get results\n",
      "DEBUG:root:loop iter get results\n",
      "DEBUG:root:loop iter get results\n",
      "DEBUG:root:loop iter get results\n",
      "DEBUG:root:loop iter get results\n",
      "DEBUG:root:loop iter get results\n",
      "DEBUG:root:loop iter get results\n",
      "DEBUG:root:loop iter get results\n",
      "DEBUG:root:loop iter get results\n",
      "DEBUG:root:loop iter get results\n",
      "DEBUG:root:loop iter get results\n",
      "DEBUG:root:loop iter get results\n",
      "DEBUG:root:loop iter get results\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m TinyLlama() \u001b[38;5;66;03m#This doesn't load the model yet and doesn't require memory\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m model\u001b[38;5;241m.\u001b[39mhost(port\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12345\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m server: \u001b[38;5;66;03m#Starts the server on port 12345\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#The model will load when it gets the first query. This can take a few minutes. If you don't host the model before, the query will crash\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#memory is freed at the end of the scope\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\nicol\\git\\lanlab2\\lanlab\\core\\module\\module.py:5\u001b[0m, in \u001b[0;36mModule.__call__\u001b[1;34m(self, struct, *args, **kwargs)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m,struct,\u001b[38;5;241m*\u001b[39margs,\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun(struct,\u001b[38;5;241m*\u001b[39margs,\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nicol\\git\\lanlab2\\lanlab\\core\\module\\models\\model.py:164\u001b[0m, in \u001b[0;36mModel.run\u001b[1;34m(self, struct)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m,struct):\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m#Loads the model if needed and if hasn't been done already\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(struct,Batch):\n\u001b[1;32m--> 164\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstruct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(struct,Sequence):\n\u001b[0;32m    166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(struct)\n",
      "File \u001b[1;32mc:\\Users\\nicol\\git\\lanlab2\\lanlab\\core\\module\\models\\model.py:218\u001b[0m, in \u001b[0;36mModel.run_batch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    217\u001b[0m     logging\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloop iter get results\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 218\u001b[0m     seq,index \u001b[38;5;241m=\u001b[39m \u001b[43mout_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m     logging\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloop iter get results 2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    220\u001b[0m     results[index] \u001b[38;5;241m=\u001b[39m seq\n",
      "File \u001b[1;32mc:\\Users\\nicol\\miniconda3\\lib\\site-packages\\multiprocess\\queues.py:116\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[0;32m    115\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(time,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonotonic\u001b[39m\u001b[38;5;124m'\u001b[39m,time\u001b[38;5;241m.\u001b[39mtime)()\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    117\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[1;32mc:\\Users\\nicol\\miniconda3\\lib\\site-packages\\multiprocess\\connection.py:260\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[1;32m--> 260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nicol\\miniconda3\\lib\\site-packages\\multiprocess\\connection.py:333\u001b[0m, in \u001b[0;36mPipeConnection._poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_got_empty_message \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m    331\u001b[0m             _winapi\u001b[38;5;241m.\u001b[39mPeekNamedPipe(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\nicol\\miniconda3\\lib\\site-packages\\multiprocess\\connection.py:882\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(object_list, timeout)\u001b[0m\n\u001b[0;32m    879\u001b[0m                 ready_objects\u001b[38;5;241m.\u001b[39madd(o)\n\u001b[0;32m    880\u001b[0m                 timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 882\u001b[0m     ready_handles \u001b[38;5;241m=\u001b[39m \u001b[43m_exhaustive_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaithandle_to_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;66;03m# request that overlapped reads stop\u001b[39;00m\n\u001b[0;32m    885\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ov \u001b[38;5;129;01min\u001b[39;00m ov_list:\n",
      "File \u001b[1;32mc:\\Users\\nicol\\miniconda3\\lib\\site-packages\\multiprocess\\connection.py:814\u001b[0m, in \u001b[0;36m_exhaustive_wait\u001b[1;34m(handles, timeout)\u001b[0m\n\u001b[0;32m    812\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m L:\n\u001b[1;32m--> 814\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWaitForMultipleObjects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;241m==\u001b[39m WAIT_TIMEOUT:\n\u001b[0;32m    816\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#HuggingFace models require to be hosted on the local computer. Lanlab provides a way to do so\n",
    "from lanlab import TinyLlama,load\n",
    "import os\n",
    "batch = load(os.path.join(\"tutorial_files\",\"my_batch\"))\n",
    "model = TinyLlama() #This doesn't load the model yet and doesn't require memory\n",
    "\n",
    "with model.host(port=52431) as server: #Starts the server on port 12345\n",
    "    out = model(batch) #The model will load when it gets the first query. This can take a few minutes. If you don't host the model before, the query will crash\n",
    "#memory is freed at the end of the scope\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(2):\n",
    "        print(\"out[\",i,\",\",j,\"] = \\n\", out[i,j].show())\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
