{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lanlab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General outlook of the framework's structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lanlab is a framework that aims at providing convenient and simple tools to help researchers build their own language model pipeline. As research in language model is developing rapidly, it is important to have a flexible framework that can be easily extended to support new models and new tasks. Lanlab is built around very simple and general concepts, and it is easy to extend it to support new models and new tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objects in lanlab are either **stuctures** or **modules** that operate on the stuctures. \n",
    "\n",
    "In native lanlab, there are three types of structures :\n",
    "- Segment\n",
    "- Sequence (lists of segments)\n",
    "- Batch (array of sequences)\n",
    "\n",
    "Let's start with the basics : Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text creates a sequence with one segment containing the given text\n",
      "len(my_text) =  1\n",
      "my_text[0].__class__ =  <class 'lanlab.core.structure.segment.Segment'>\n",
      "my_text[0].format() =  I like chocolate\n",
      "\n",
      "The format() method returns a formatting of the text of the segment but it can also be used on sequences concatenating the formatting of all its segments\n",
      "my_text.format() =  I like chocolate\n"
     ]
    }
   ],
   "source": [
    "#Creating text segments\n",
    "from lanlab import Text\n",
    "\n",
    "my_text = Text(\"I like chocolate\")\n",
    "\n",
    "print(\"Text creates a sequence with one segment containing the given text\")\n",
    "print(\"len(my_text) = \", len(my_text))\n",
    "print(\"my_text[0].__class__ = \", my_text[0].__class__)\n",
    "print(\"my_text[0].format() = \", my_text[0].format())\n",
    "print()\n",
    "print(\"The format() method returns a formatting of the text of the segment but it can also be used on sequences concatenating the formatting of all its segments\")\n",
    "print(\"my_text.format() = \", my_text.format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts can be concatenated with the + operator\n",
      "my_text3 = my_text + my_text2\n",
      "my_text3.format() =  I like chocolate and I like ice cream\n",
      "\n",
      "my_text3 contains 2 segments\n",
      "len(my_text3) =  2\n",
      "my_text3[0].format() =  I like chocolate\n",
      "my_text3[1].format() =   and I like ice cream\n"
     ]
    }
   ],
   "source": [
    "#Concatenating segments\n",
    "my_text2 = Text(\" and I like ice cream\")\n",
    "my_text3 = my_text + my_text2\n",
    "\n",
    "print(\"Texts can be concatenated with the + operator\")\n",
    "print(\"my_text3 = my_text + my_text2\")\n",
    "print(\"my_text3.format() = \", my_text3.format())\n",
    "\n",
    "print()\n",
    "print(\"my_text3 contains 2 segments\")\n",
    "print(\"len(my_text3) = \", len(my_text3))\n",
    "print(\"my_text3[0].format() = \", my_text3[0].format())\n",
    "print(\"my_text3[1].format() = \", my_text3[1].format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Editing segments data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segments store a lot of data more than text ['text', 'origin', 'tags', 'info', 'format', 'model', 'tokens', 'logp', 'top_logp', 'finish_reason', 'logits']\n",
      "segment['text'] =  I like chocolate -- is the text of the segment\n",
      "segment['format'] =  {'chat': '[text]', 'completion': '[text]'} -- is the formatting system of the segment. LLMs will get the completion formatting or chat formatting depending on their nature. This formating will just replace the [text] with the segment's text\n",
      "\n",
      "Let's update it : {'chat': '{[text]}', 'completion': '[text]'}\n",
      "segment.format('completion') =  I like chocolate -- is the completion formatting\n",
      "segment.format('chat') =  {I like chocolate} -- is the chat formatting\n",
      "This makes it possible to use the same segment and to adapt the prompting for chat and completion models when needed\n",
      "\n",
      "segment['origin'] =  user -- is the origin of the segment. It is about who generated this segment. It can be a user, a model or the system. This will then be communicated to chat models. This information isn't used by completion models\n",
      "segment['model'] =  None -- is the name of the model that generated this segment. It is put to None if the user created it\n",
      "\n",
      "There are many more that will be discussed later as they are not important for now\n"
     ]
    }
   ],
   "source": [
    "#Segment data\n",
    "segment = my_text3[0]\n",
    "\n",
    "print(\"Segments store a lot of data more than text\",segment.keys())\n",
    "print(\"segment['text'] = \", segment['text'],\"-- is the text of the segment\")\n",
    "print(\"segment['format'] = \", segment['format'],\"-- is the formatting system of the segment. LLMs will get the completion formatting or chat formatting depending on their nature. This formating will just replace the [text] with the segment's text\")\n",
    "\n",
    "print()\n",
    "segment['format']['chat'] = '{[text]}'\n",
    "print(\"Let's update it :\",segment['format'])\n",
    "print(\"segment.format('completion') = \", segment.format('completion'),\"-- is the completion formatting\")\n",
    "print(\"segment.format('chat') = \", segment.format('chat'),\"-- is the chat formatting\")\n",
    "print(\"This makes it possible to use the same segment and to adapt the prompting for chat and completion models when needed\")\n",
    "\n",
    "print()\n",
    "print(\"segment['origin'] = \", segment['origin'], \"-- is the origin of the segment. It is about who generated this segment. It can be a user, a model or the system. This will then be communicated to chat models. This information isn't used by completion models\")\n",
    "print(\"segment['model'] = \", segment['model'], \"-- is the name of the model that generated this segment. It is put to None if the user created it\")\n",
    "\n",
    "print()\n",
    "print(\"There are many more that will be discussed later as they are not important for now\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules can be called on sequences to complete them with an additional segment\n",
      "new_text = model(my_text3) =  I like chocolate and I like ice creamThat's great! Chocolate and ice cream make a delicious combination. Do you have\n",
      "\n",
      "Chats are not very readable with the format method as it concatenates all the segments. Let's use the show method instead to print a human readable version of the sequence\n",
      "print(new_text) = \n",
      "user(None): I like chocolate\n",
      "user(None):  and I like ice cream\n",
      "assistant(gpt-3.5-turbo-0613): That's great! Chocolate and ice cream make a delicious combination. Do you have\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calling a model\n",
    "from lanlab import GPT35\n",
    "model = GPT35()\n",
    "\n",
    "print(\"Modules can be called on sequences to complete them with an additional segment\")\n",
    "my_text4 = model(my_text3)\n",
    "print(\"new_text = model(my_text3) = \", my_text4.format())\n",
    "\n",
    "print()\n",
    "print(\"Chats are not very readable with the format method as it concatenates all the segments. Let's use the show method instead to print a human readable version of the sequence\")\n",
    "print(\"print(new_text) = \")\n",
    "print(my_text4.show())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some clarifications are needed here to explain how lanlab handles text and present them to the models. For completion models, the model will get as input q string : ```sequence.format(\"completion\")```. For chat models a chat is required. The ```segment.format(\"chat\")``` for the content along with ```segment.['origin']``` for the origin of the text are used to feed the model. Each segment is a message, a sequence being a conversation. The conversation can include system messages as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuring a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules have their own configurations like segments\n",
      "model.config.keys() =  ['temperature', 'max_tokens', 'top_p', 'stop', 'logit_bias']\n",
      "\n",
      "model.config['max_tokens'] =  16 -- is the maximum number of tokens of the generated text\n",
      "model.config['temperature'] =  0.7 -- is the temperature of the generated text\n",
      "model.config['top_p'] =  1 -- is the top_p of the generated text\n",
      "The other parameters are more complex and will be discussed later\n"
     ]
    }
   ],
   "source": [
    "#Configuring a model\n",
    "print(\"Modules have their own configurations like segments\")\n",
    "print(\"model.config.keys() = \", model.config.keys())\n",
    "\n",
    "print()\n",
    "print(\"model.config['max_tokens'] = \", model.config['max_tokens'], \"-- is the maximum number of tokens of the generated text\")\n",
    "print(\"model.config['temperature'] = \", model.config['temperature'], \"-- is the temperature of the generated text\")\n",
    "print(\"model.config['top_p'] = \", model.config['top_p'], \"-- is the top_p of the generated text\")\n",
    "\n",
    "print(\"The other parameters are more complex and will be discussed later\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In native lanlab there are 3 preimplemented bridges to other framework : OpenAI, HuggingFace and Palm (coming soon)\n",
      "In the future more will be added and the user is free to add his own\n",
      "The available models are :\n",
      "OPENAI\n",
      "  ada\n",
      "  text-ada-001\n",
      "  babbage\n",
      "  text-babbage-001\n",
      "  curie\n",
      "  text-curie-001\n",
      "  davinci\n",
      "  davinci-instruct-beta\n",
      "  text-davinci-001\n",
      "  code-davinci-002\n",
      "  text-davinci-002\n",
      "  text-davinci-003\n",
      "  gpt-3.5-turbo-instruct\n",
      "  gpt-3.5-turbo-instruct-0914\n",
      "  davinci-002\n",
      "  babbage-002\n",
      "  gpt-3.5-turbo\n",
      "  gpt-3.5-turbo-0613\n",
      "  gpt-3.5-turbo-0301\n",
      "  gpt-3.5-turbo-16k\n",
      "  gpt-3.5-turbo-16k-0613\n",
      "  gpt-4\n",
      "  gpt-4-0314\n",
      "  gpt-4-0613\n",
      "  gpt-4-1106-preview\n",
      "  gpt-4-vision-preview\n",
      "\n",
      "HuggingFace\n",
      "  None\n",
      "  llama-7b\n",
      "  llama-13b\n",
      "  alpaca-7b\n",
      "  wizard-7b\n",
      "  vicuna-7b-v1.1\n",
      "  vicuna-7b-v1.3\n",
      "  vicuna-7b-v1.5\n",
      "  vicuna-13b-v1.1\n",
      "  vicuna-13b-v1.3\n",
      "  vicuna-13b-v1.5\n",
      "  baize-7b\n",
      "  guanaco-7b\n",
      "  tiny-llama-fast-tokenizer\n",
      "  llama-2-7b\n",
      "  llama-2-13b\n",
      "  llama-2-7b-hf\n",
      "  llama-2-13b-hf\n",
      "  Orca-2-7b\n",
      "  Orca-2-13b\n",
      "  None\n",
      "  bloom-3b\n",
      "  bloom-7b\n",
      "  bloomz-3b\n",
      "  bloomz-7b\n",
      "  None\n",
      "  pythia-2.8b\n",
      "  pythia-6.9b\n",
      "  pythia-12b\n",
      "  dolly-v2-3b\n",
      "  dolly-v2-7b\n",
      "  dolly-v2-12b\n",
      "  None\n",
      "  phi-1\n",
      "  phi-1_5\n",
      "  None\n",
      "  cerebras-111m\n",
      "  cerebras-256m\n",
      "  cerebras-590m\n",
      "  cerebras-1.3b\n",
      "  cerebras-2.7b\n",
      "  cerebras-6.7b\n",
      "  cerebras-13b\n",
      "  None\n",
      "  Qwen-1_8B\n",
      "  Qwen-7B\n",
      "  Qwen-14B\n",
      "  Qwen-72B\n",
      "  None\n",
      "  Yi-6B\n",
      "  Yi-34B\n",
      "  None\n",
      "  Mistral-7B-v0.1\n",
      "  Mistral-7B-Instruct-v0.1\n",
      "  zephyr-7b-alpha\n",
      "  zephyr-7b-beta\n",
      "  None\n",
      "  fuyu-8b\n",
      "  None\n",
      "  falcon-rw-1b\n",
      "  falcon-rw-7b\n",
      "  falcon-7b\n",
      "  falcon-7b-instruct\n",
      "  falcon-40b\n",
      "  falcon-40b-instruct\n",
      "  None\n",
      "  opt-125m\n",
      "  opt-350m\n",
      "  opt-1.3b\n",
      "  opt-2.7b\n",
      "  opt-6.7b\n",
      "  opt-13b\n",
      "  opt-30b\n",
      "  opt-66b\n",
      "  None\n",
      "  openchat_v2\n",
      "  openchat_v2_w\n",
      "  openchat_v3.1\n",
      "  openchat_v3.2\n",
      "  openchat_v3.2_super\n",
      "  openchat_3.5\n",
      "  None\n",
      "  Starling-LM-7B-alpha\n",
      "  None\n",
      "  neural-chat-7b-v3\n",
      "  neural-chat-7b-v3-1\n",
      "  None\n",
      "  causallm-7b\n",
      "  causallm-14b\n",
      "  None\n",
      "  tigerbot-7b-base\n",
      "  tigerbot-7b-base-v1\n",
      "  tigerbot-7b-base-v2\n",
      "  tigerbot-7b-sft-v1\n",
      "  tigerbot-7b-sft-v2\n",
      "  tigerbot-7b-chat\n",
      "  tigerbot-13b-base-v1\n",
      "  tigerbot-13b-base-v2\n",
      "  tigerbot-13b-chat-v1\n",
      "  tigerbot-13b-chat-v2\n",
      "  tigerbot-13b-chat-v3\n",
      "  tigerbot-13b-chat-v4\n",
      "  OpenHermes-7B\n",
      "  OpenHermes-13B\n",
      "  OpenHermes-2-Mistral-7B\n",
      "  OpenHermes-2.5-Mistral-7B\n",
      "  None\n",
      "  NexusRaven-13B\n",
      "  NexusRaven-V2-13B\n",
      "\n",
      "The classes can be used to create new models and are referenced in lanlab.get_openai_model_classes() and lanlab.get_hf_model_classes()\n",
      "Please note that OPENAI models access require an API key and HuggingFace models require hosting them on your own computer with all their own requirements\n"
     ]
    }
   ],
   "source": [
    "#Available models\n",
    "from lanlab import get_hf_model_classes, get_openai_model_classes\n",
    "print(\"In native lanlab there are 3 preimplemented bridges to other framework : OpenAI, HuggingFace and Palm (coming soon)\")\n",
    "print(\"In the future more will be added and the user is free to add his own\")\n",
    "print(\"The available models are :\")\n",
    "print(\"OPENAI\")\n",
    "for m in get_openai_model_classes():\n",
    "    print(\" \",m().engine)\n",
    "\n",
    "print()\n",
    "print(\"HuggingFace\")\n",
    "for m in get_hf_model_classes():\n",
    "    print(\" \",m().engine)\n",
    "\n",
    "print()\n",
    "print(\"The classes can be used to create new models and are referenced in lanlab.get_openai_model_classes() and lanlab.get_hf_model_classes()\")\n",
    "print(\"Please note that OPENAI models access require an API key and HuggingFace models require hosting them on your own computer with all their own requirements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches are arrays of sequences\n",
      "batch.shape =  (5, 2)\n",
      "batch[0,0] =  Sequence(\n",
      ")\n",
      "\n",
      "Batched sequences can be accessed with the same syntax as numpy arrays and can be filled with your own sequences\n",
      "batch[0,0] =  I like chocolate\n",
      "Models can be called on top of batches and will process sequences in parallel up to a given batch size set_number_workers(n). This speeds up the process a lot.\n",
      "batch_completed[ 0 , 0 ] =  0+0=0\n",
      "batch_completed[ 0 , 1 ] =  0+1=1\n",
      "batch_completed[ 1 , 0 ] =  1+0=1\n",
      "batch_completed[ 1 , 1 ] =  1+1=2\n",
      "batch_completed[ 2 , 0 ] =  2+0=2\n",
      "batch_completed[ 2 , 1 ] =  2+1=2 + 1 = 3\n",
      "batch_completed[ 3 , 0 ] =  3+0=3\n",
      "batch_completed[ 3 , 1 ] =  3+1=4\n",
      "batch_completed[ 4 , 0 ] =  4+0=4\n",
      "batch_completed[ 4 , 1 ] =  4+1=5\n"
     ]
    }
   ],
   "source": [
    "#Batches\n",
    "from lanlab import Batch,set_number_workers\n",
    "print(\"Batches are arrays of sequences\")\n",
    "batch = Batch((5,2)) #Creates a batch of 5x2 empty sequences\n",
    "print(\"batch.shape = \", batch.shape)\n",
    "print(\"batch[0,0] = \", batch[0,0])\n",
    "\n",
    "print()\n",
    "print(\"Batched sequences can be accessed with the same syntax as numpy arrays and can be filled with your own sequences\")\n",
    "batch[0,0] = my_text\n",
    "print(\"batch[0,0] = \", batch[0,0].format())\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(2):\n",
    "        batch[i,j] = Text(str(i)+\"+\"+str(j)+\"=\")\n",
    "\n",
    "set_number_workers(10) #Set the number of queries to be sent in parallel to the model\n",
    "print(\"Models can be called on top of batches and will process sequences in parallel up to a given batch size set_number_workers(n). This speeds up the process a lot.\")\n",
    "batch_completed = model(batch)\n",
    "for i in range(5):\n",
    "    for j in range(2):\n",
    "        print(\"batch_completed[\",i,\",\",j,\"] = \", batch_completed[i,j].format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential is a container that can be used to chain modules\n",
      "run\n",
      "results[ 0 , 0 ] = \n",
      " user(None): 0+0=\n",
      "assistant(gpt-3.5-turbo-0613): 0\n",
      "user(None): Are you sure ?\n",
      "assistant(gpt-3.5-turbo-0301): Yes, I am sure. When you add zero to zero, you get zero as a result.\n",
      "\n",
      "\n",
      "results[ 0 , 1 ] = \n",
      " user(None): 0+1=\n",
      "assistant(gpt-3.5-turbo-0613): 1\n",
      "user(None): Are you sure ?\n",
      "assistant(gpt-3.5-turbo-0301): Yes, as an AI language model, I am programmed to perform basic arithmetic operations, and the sum of 0 and 1 is always 1.\n",
      "\n",
      "\n",
      "results[ 1 , 0 ] = \n",
      " user(None): 1+0=\n",
      "assistant(gpt-3.5-turbo-0613): 1\n",
      "user(None): Are you sure ?\n",
      "assistant(gpt-3.5-turbo-0301): Yes, I am an AI language model and I am sure that 1 + 0 equals 1.\n",
      "\n",
      "\n",
      "results[ 1 , 1 ] = \n",
      " user(None): 1+1=\n",
      "assistant(gpt-3.5-turbo-0613): 2\n",
      "user(None): Are you sure ?\n",
      "assistant(gpt-3.5-turbo-0301): Yes, as an AI language model, I am programmed to perform basic arithmetic operations accurately, and 1+1 always equals 2.\n",
      "\n",
      "\n",
      "results[ 2 , 0 ] = \n",
      " user(None): 2+0=\n",
      "assistant(gpt-3.5-turbo-0613): 2\n",
      "user(None): Are you sure ?\n",
      "assistant(gpt-3.5-turbo-0301): Yes, I am sure. 2+0=2.\n",
      "\n",
      "\n",
      "results[ 2 , 1 ] = \n",
      " user(None): 2+1=\n",
      "assistant(gpt-3.5-turbo-0613): 3\n",
      "user(None): Are you sure ?\n",
      "assistant(gpt-3.5-turbo-0301): Yes, I am sure. 2+1 equals 3.\n",
      "\n",
      "\n",
      "results[ 3 , 0 ] = \n",
      " user(None): 3+0=\n",
      "assistant(gpt-3.5-turbo-0613): 3\n",
      "user(None): Are you sure ?\n",
      "assistant(gpt-3.5-turbo-0301): Yes, I am sure. 3 plus 0 equals 3.\n",
      "\n",
      "\n",
      "results[ 3 , 1 ] = \n",
      " user(None): 3+1=\n",
      "assistant(gpt-3.5-turbo-0613): 4\n",
      "user(None): Are you sure ?\n",
      "assistant(gpt-3.5-turbo-0301): Yes, as an AI language model, I am certain that 3+1 equals 4.\n",
      "\n",
      "\n",
      "results[ 4 , 0 ] = \n",
      " user(None): 4+0=\n",
      "assistant(gpt-3.5-turbo-0613): 4\n",
      "user(None): Are you sure ?\n",
      "assistant(gpt-3.5-turbo-0301): Yes, as an AI language model, I am programmed to perform basic arithmetic operations accurately and with precision. Therefore, 4+0 is definitely equal to \n",
      "\n",
      "\n",
      "results[ 4 , 1 ] = \n",
      " user(None): 4+1=\n",
      "assistant(gpt-3.5-turbo-0613): 5\n",
      "user(None): Are you sure ?\n",
      "assistant(gpt-3.5-turbo-0301): As an AI language model, I am programmed to provide accurate responses. Yes, I am sure that 4+1=5.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Sequential\n",
    "from lanlab import Sequential\n",
    "print(\"Sequential is a container that can be used to chain modules\")\n",
    "from lanlab import GPT35_0613, GPT35_0301\n",
    "\n",
    "#This will first call the model, then append the text \"Are you sure ?\" to the completion and then call the model again. TO make it more convenient, despite text not being a module it is interpreted as a module that concatenates the text to the sequence.\n",
    "model1 = GPT35_0613()\n",
    "model2 = GPT35_0301().configure(max_tokens=32)\n",
    "seq = Sequential(model1, \"Are you sure ?\", model2)\n",
    "\n",
    "results = seq(batch)\n",
    "for i in range(5):\n",
    "    for j in range(2):\n",
    "        print(\"results[\",i,\",\",j,\"] = \\n\", results[i,j].show())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lanlab import save, load\n",
    "import os\n",
    "\n",
    "#Saving a structure (segment, sequence, batch)\n",
    "save(batch,os.path.join(\"tutorial_files\",\"my_batch\"))\n",
    "\n",
    "#Loading a structure (segment, sequence, batch)\n",
    "loaded_batch = load(os.path.join(\"tutorial_files\",\"my_batch\"))\n",
    "\n",
    "#Saving automatically in Sequential\n",
    "seq(batch,os.path.join(\"tutorial_files\",\"my_seq2\"))\n",
    "\n",
    "#If you run it again, it will load the saved file instead of running the model again\n",
    "results = seq(batch,os.path.join(\"tutorial_files\",\"my_seq2\")) #Another computation has the same path -> load it instead of running it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading HF models\n",
    "\n",
    "To run this cell you need to ```git clone https://huggingface.co/fxmarty/tiny-llama-fast-tokenizer``` and to follow the instructions in the lanlab README repository to install the model. This is a dummy model that completes with random token but is very convenient to verify that the pipeline works before running it on larger models on a GPU cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'lanlab.core.module.models.hf_models' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug: * Running on all addresses (0.0.0.0)\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      " * Running on http://127.0.0.1:52431\n",
      " * Running on http://10.188.129.27:52431 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model dtype torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daetheys/anaconda3/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "Using pad_token, but it is not set yet.\n",
      "INFO:werkzeug:127.0.0.1 - - [12/Dec/2023 15:17:59] \"POST /completions HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [12/Dec/2023 15:17:59] \"POST /completions HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [12/Dec/2023 15:17:59] \"POST /completions HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [12/Dec/2023 15:17:59] \"POST /completions HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [12/Dec/2023 15:17:59] \"POST /completions HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [12/Dec/2023 15:17:59] \"POST /completions HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [12/Dec/2023 15:17:59] \"POST /completions HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [12/Dec/2023 15:17:59] \"POST /completions HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [12/Dec/2023 15:17:59] \"POST /completions HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [12/Dec/2023 15:17:59] \"POST /completions HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out[ 0 , 0 ] = \n",
      " user(None): 0+0=\n",
      "assistant(TLLA):  second otroROP blah goldengetNameioǧheit spark accompgas rein орган soupюза\n",
      "\n",
      "\n",
      "out[ 0 , 1 ] = \n",
      " user(None): 0+1=\n",
      "assistant(TLLA):  приня Top용ew ocupamaут участ mens располо illorigine hockey сталиждён synd\n",
      "\n",
      "\n",
      "out[ 1 , 0 ] = \n",
      " user(None): 1+0=\n",
      "assistant(TLLA):  precise имеет площа Rightktet\".$ зда wordsasy nombreux Even SongetailedVariable Arc th\n",
      "\n",
      "\n",
      "out[ 1 , 1 ] = \n",
      " user(None): 1+1=\n",
      "assistant(TLLA): xf智 Nebenвсямом catch pride Agricult який Reinogo regiónlookignon Young fruit\n",
      "\n",
      "\n",
      "out[ 2 , 0 ] = \n",
      " user(None): 2+0=\n",
      "assistant(TLLA): bareca canción Fail ocean════łużulyPer dim opposedrepabadciale accomplishedteenth\n",
      "\n",
      "\n",
      "out[ 2 , 1 ] = \n",
      " user(None): 2+1=\n",
      "assistant(TLLA):  TováFig customers більUMN限 enjoyeduralgetElementんпростра culture Erstcribed帝 Region\n",
      "\n",
      "\n",
      "out[ 3 , 0 ] = \n",
      " user(None): 3+0=\n",
      "assistant(TLLA):  ВелиO posible事 xml estim badly Spirit Kostouri doub XIII DouEL repla trab\n",
      "\n",
      "\n",
      "out[ 3 , 1 ] = \n",
      " user(None): 3+1=\n",
      "assistant(TLLA):  String 'UID dispatchських tested� !== mesureára达 Sep pin Bash acht habitat\n",
      "\n",
      "\n",
      "out[ 4 , 0 ] = \n",
      " user(None): 4+0=\n",
      "assistant(TLLA):  Spr compilationak shoutḫ cirmathopuestlocal nederbörd median Load conjuntowie2 flag\n",
      "\n",
      "\n",
      "out[ 4 , 1 ] = \n",
      " user(None): 4+1=\n",
      "assistant(TLLA): }}}\\ nacTEXTffenFig inquSA Fue Martín gentle proportion或 Buc subscri regardless Junior\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#HuggingFace models require to be hosted on the local computer. Lanlab provides a way to do so\n",
    "from lanlab import TinyLlama,load\n",
    "import os\n",
    "batch = load(os.path.join(\"tutorial_files\",\"my_batch\"))\n",
    "model = TinyLlama() #This doesn't load the model yet and doesn't require memory\n",
    "\n",
    "with model.host(port=52431) as server: #Starts the server on port 12345\n",
    "    out = model(batch) #The model will load when it gets the first query. This can take a few minutes. If you don't host the model before, the query will crash\n",
    "#memory is freed at the end of the scope\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(2):\n",
    "        print(\"out[\",i,\",\",j,\"] = \\n\", out[i,j].show())\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "812eebe2a6551e6686edfa30fbadf2a874e9eafd53d88484fa1f6e6b1a5bacf0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
